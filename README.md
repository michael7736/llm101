# LLM101课程介绍
# 综述

       基于大语言模型发展出的生成式AI正火爆全球，并以几何级数速度发展壮大。很快，这种通用人工智能将改变我们周围的一切。尤其是工业领域/商业领域乃至与科学研究学术领域，都将被从根本上基于AI重新构建。这些行业的变化，因此AI也影响到我们每一个人的未来。不论是专门的计算机人工智能领域，还是从事各行各业的生产和研究，理解并掌握生成式AI的基本原理和实践，都是必备技能。因此在AI学习和教育培训领域也涌现出了很多的课程与机构。

       然而在繁荣的背后，却存在着很多问题。首先，AI发展已经有几十年的历史，但围绕大语言模型展开的生成式AI技术却是很新的领域。尤其是国内，对这方面的技术理解与实践还处于非常初级阶段，这种与国际上技术理解的信息差，导致了AI技术内容比较混乱，我们要学什么？从那儿开始，另很多人困惑。其次，AI领域技术极其复杂，涉及的技术领域包括计算机科学、编程语言、数学、统计学、认知心理学、机器学习、神经网络、数据科学、图形学，此外还包含大量的创新性的论文、方法论、理论体系，以及围绕该技术的各种创新型工具、工业框架。这让一个新手进入这一领域非常困难。

        目前，国内针对AI领域的教培存在两个主要流派。一个是理论派，主要来自高校和传统AI公司，重点数学原理、机器学习、数据科学等理论。该流派主要的问题是理论过于晦涩、难于理解，同时存在大量的陈旧的理论铺垫，其效率与实用性脱节，无法激发学员对AI的价值和兴趣。 另外一个是动手派，这部分强调实际代码和工具的构建、使用。这部分主要集中在一些具有工业经验的研发人员。这部分因其直接输出AI的效果，比较震撼，容易激发出学员的热情，是一个比较好的出发点。但动手派，如果只局限于具体工具的使用，而不注重对代码及背后的理论因素的理解和铺垫，就导致学员仅仅局限于特定场景的实战，而不具备真实的创造能力和解决问题的能力。最后变成比较低层次的执行者。

       为了推广先进的AI技术在国内的落地，我们需要更多的人才涌入这个领域，提升国人在AI领域的竞争力，我们开发了《动手学大模型课程》。参照国内、外相对成熟的生成式AI的课程和理论实践经验。围绕实际动手的模型训练项目、实战代码和工具部署，在实战中对上下文理论进行讲解、剖析。按照从实战到理论， 再从理论到实战的循环。让学员真正具备大模型和AI的理论和实践能力。从而为即将到来的人工智能风潮培养不同层面的人才。

通过学习这个课程，学员可以达成以下目标：

- 深入理解生成式AI，了解大语言模型 项目生命周期的全景。包括数据收集，模型和算法选择性能评估和部署；
- 详细介绍作为所有llm基础的transformer架构，包括在GPT、BERT、llama等模型的应用，变种；
- 通过预训练、微调和对齐模型等实际的案例，让学员更深入的理解理论同时，具有实际的工程构建能力，可以端到端的训练、构建完整的大模型，实现真正的入门。学习采用关键的技术方法和工具对模型进行微调，并按需调教出自己的大语言模型；
- 可以使用大模型构建出常规的AI应用，包含ChatBot， RAG 知识库、和AI Agents。
- 了解生成式AI在学术界和工业界的现状，权威参考与研究成果跟踪。
- 在学习过程中，动手实践过程中，解决一个个难题过程中不断的深化理论和实际的技术。

通过掌握以上知识和技能，开发者可以在未来的个人研究和工作领域充分发挥数据和人工智能的力量，创造出更多的创新的项目，推进科技进步。

# 受众与准备

       这是一门入门课程，也可以作为各领域运用AI的实践课。原则上不需要有任何前置条件，我们也会在课程中穿插一些前置基础的入门课程和参考学习资源。本着Learning by doing(动手学)的原则，将通过大量的实验，代码对大语言模型以及相关的知识进行讲解。当然，如果你具有以下经验背景，将让你更快速掌握核心技术，理解更深入。动手学AI只是起点，终点无上限。

1. 数学基础：面向大学理工科本科或同等学力学员，建议学习过理解线性代数、微积分、统计学相关课程；
2. 时间投入：每周至少投入10小时学习与实验，0基础者翻倍，当然，你进步的速度，决定于你的学习浓度和密度；
3. 硬件要求：使用云资源者最低要求任意操作系统的笔记本；如需要本地实验，建议x86系统16G内存，GPU显存16G以上4080，或4090，安装ubuntu linux。苹果系统macbook m1或m2芯片，内存32G以上；
4. 建议了解类Unix 操作系统基本使用，如Ubuntu，或者Mac OS，尤其是shell基本命令；
5. 英语通过4级，建议可独立阅读英文文章；
6. Python 基础编程，会构建、使用函数，一些基础库和数据科学库；
7. 机器学习基础概念，如监督学习，无监督学习，损失函数，机器学习数据处理基础（和数据科学中略有不同）；
8. 深度学习和神经网络基础知识；
9. NLP了解自然语言处理。

如果想了解大语言模型的相关参考资料，课程会给出阅读清单。关于深度学习，机器学习以及数据科学相关的学习材料，实验材料，也将给出参考链接。

# 课程特点

- 简单高效：大模型说是非常复杂的技术，要想快速入门，甚至脱颖而出，必须付出巨大的努力，包含时间和心力，学习效率这里非常关键。学员需要抓住核心概念和技术，剔除一些内、外部的干扰，本课程抓主要矛盾，先易后难，层层递进，每一个小节专注一个知识点，并在后面的实践中反复验证。
- 实用：强调动手能力，理论与实践相结合。所有的技术讲解都尽可能结合相应实现代码；课程将结合实际模型训练任务、项目实验；包含其行业最流行的开源工具、框架；各种Sota测试工具、数据。学成后可直接应用到未来的行业工作中，同时在实验中，对背后的理论机制进行讲解，具有直接感受；
- 专业：站在巨人的肩膀上，本课程为开源项目，目标是让学员快速跟进最先进的技术，因此，会参照全球最先进、最专业和资源，在每一个领域，首先会有一个参考链接。对标学术界如斯坦福、哈弗、MIT，工业界如OpenAI、谷歌、Meta以及huggingface，以及Github上的主流的开源工具，参考AI界顶级科学家的论文与观点的分析整合，确保与世界AI同步，；
- 生成式AI全景和突出重点：AI是及其复杂的技术，并且飞速的发展变化。面对庞杂的AI技术，形成统一、全景的认知，保持正确的方向与成长路径对于学员保持长期、可持续学习研究能力尤其重要。本课程在一开始就帮助学员形成一个llm的整体技术架构与生态，并在后续的学习中层层深入。并在学习中不断完善知识、技术拼图。一期重点在基于大语言模型的微调与行业适配，可低门槛培养出大批量的工程技术人才。随着学员个人能力提高与研究的深入，可培养出具有创新能力的高级AI人才；
- 低门槛、梯度进阶：课程目标是推动AI在各行各业的广泛应用，目标受众是通用技术人才。只要你有强烈的产业变革需求、对技术和AI有强烈的热情和兴趣，本课程将为你准备入门需要的一切准备活动。

# 课程目录

预期整个课程12次课。课件与课程目录将随后给出。以下为初步目录。

1. 第0课：课程环境准备（与课程同步进行）
    1. colab环境简介（需要网络）
    2. linux 环境准备（ubuntu + gpu配置）
    3. mac OS环境准备
    4. jupyter notebook（lab）介绍
    5. Docker环境准备和介绍
2. 第1课：课程介绍、框架、全景介绍
    1. 大模型的前世今生-感性认识大模型和生成式AI，AGI
    2. 大语言模型学习内容介绍：两条主线，一条辅助线
    3. 大语言模型学习方法介绍
3. 第2课：Transformer internal
    1. transformer整体架构分层介绍
    2. 手写Transformer代码
    3. 实战：利用huggingface transformer初探，如何运行一个大模型
    4. BERT解析
    5. GPT2 解析
    6. llama3解析
    7. Mistral MOE解析
    8. 实验：用工具探索llm 架构
4. 第3课：大模型的使用Prompt Engineering
    1. 大模型的启动姿势：HF transformers，ollama，API
    2. 大模型的任务介绍
    3. Prompt Engineering介绍
    4. Prompt Engineering 实战
    5. 实验：ChatGPT UI，OpenAI API
    6. 实验：利用transformers加载，使用大模型
    7. 实验：ollama 运行大模型
    8. 睡眼：用vllm、tgi运行大模型推理
5. 第4课：模型预训练
    1. 预训练介绍：整体流程、组件、Why预训练
    2. 预训练数据处理
    3. 分词器Tokenizer介绍
    4. 嵌入与嵌入模型
    5. 预训练架构设计与超参数
    6. 大模型训练工具一览
    7. 实验：训练tokenizer
    8. 实验：预训练一个BERT
    9. 实验：预训练GPT2
    10. 实验：用继续预训练，训练llama3.1中文
    11. 实验：继续预训练，训练编程大模型
    12. 实验：利用unsloth 进行模型训练
    13. 实验：用llama-factory 模型
    14. 专题：大规模并行训练原理和工具介绍
6. 第5课：模型微调-指令微调
    1. 微调技术全景-微调类型和架构介绍？
    2. 如何选择基础模型（底模），模型的指标和评估
    3. 基于hugging-face PEFT实现不同类型的微调
    4. 微调数据准备
    5. Trl，大模型微调工具介绍
    6. 微调工具介绍：unsloth, llama-factory进行微调
    7. Lora 训练
    8. 实验：llama3.1微调实战
    9. 实验：Qwen2微调实战
    10. 实验：大模型评测
7. 第8课：大模型对齐和RLHF
    1. 对齐的价值：为什么我们需要它？ 和预训练、FT的对比来解释。大模型可靠性问题？ 偏见、错误、和鲁棒性、安全隐私，面对的是开放性问题。
    2. RLHF的流程和架构
    3. 准备偏好数据
    4. 奖励模型和训练
    5. 强化学习进行基础模型训练
    6. 实验：用Pipeline进行RLHF
    7. 实验：用TensorBoard跟踪模型训练
8. 第9课：大模型的评测
    1. 评估的重要性和基础概念
    2. 大模型评估方法介绍：人工评估、基准测试、
    3. 实验：模型Benchmark测试
    4. 实验：手工评测、对比不同的模型的性能
9. 第10课：大模型的部署
    1. 模型推理的关键指标和相关技术介绍
    2. 推理引擎介绍
    3. llmOps工具介绍
    4. 大模型的性能优化方案介绍：量化、蒸馏、剪枝、**自适应优化算法**
    5. 实验：TGI、VLLM 和TensorRT模型的性能对比评测
10. 第11课：RAG企业应用开发范式
    1. Why RAG？ 
    2. RAG基础架构介绍
        1. RAG数据准备
        2. RAG数据分块Chunk
        3. 嵌入模型选择和训练
        4. 向量数据库和搜索
        5. 生成模型选择
        6. 检索策略和优化
    3. RAG评估
    4. RAG开发框架介绍：第一代：LangChain， LlamaIndex, 第二代：Dify、Ragflow
    5. 实验：
11. 第12课：AI Agents
    1. 
12. 第13课：多模态大模型
13. 当前的范式面临的问题
14. 大模型研究的新方向
15. 大模型工业化方向-适配
16. 深入拓展学习

# 课程开发者介绍

机构介绍：MetaDataSec 通用人工智能实验室(AGI Lab)

愿景：

推动AGI在各行各业落地，推动科技创新与产业智能化革命。

途径：


# 参考资料

1. Generative AI with Large Language Models
    1. https://www.coursera.org/learn/generative-ai-with-llms/home/week/1

Deeplearning.AI出品，质量没问题，比较全面从理论，到实践的课程，目前还没看完，但主要问题是理论过多，实践偏少，学习有一定门槛。 需要补充大量的开源工具箱，和代码层面的实现。

1. Natural Language Processing with Classification and Vector Spaces
    1. https://www.coursera.org/learn/classification-vector-spaces-in-nlp?specialization=natural-language-processing
2. 斯坦福人工智能系列课程
    1. https://online.stanford.edu/artificial-intelligence

包含机器学习、深度学习、生成式AI、自然语言处理等，是最全面的AI系列有利于进行深度和广度拓展

1. 清华大学的大模型线上课程
    1. https://www.openbmb.org/community/course
2. MIT deep learning 课程
    1. http://introtodeeplearning.com/
3. Google 生成式AI课程
    1. https://www.cloudskillsboost.google/journeys/118
4. 生成式AI相关论文、技术文集和参考信息汇总
    1. https://github.com/Hannibal046/Awesome-LLM
5. LLMsPracticalGuide
    1. https://github.com/Mooler0410/LLMsPracticalGuide
6. CS25: Transformers United V2
    1. https://web.stanford.edu/class/cs25/
7. [Yao Fu] 预训练，指令微调，对齐，专业化：论大语言模型能力的来源
    1. https://www.bilibili.com/video/BV1Qs4y1h7pn/?spm_id_from=333.337.search-card.all.click&vd_source=1e55c5426b48b37e901ff0f78992e33f
8. Instruction finetuning and RLHF lecture (NYU CSCI 2590)
    1. https://www.youtube.com/watch?v=zjrM-MW-0y0
9. State of GPT by Andrej Karpathy
    1. https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2
10. [Andrej Karpathy] Let's build GPT: from scratch, in code, spelled out
    1. https://www.youtube.com/watch?v=kCc8FmEb1nY
11. How to code The Transformer in Pytorch
    1. https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec
12. Huggingface NLP course
    1. https://huggingface.co/learn/nlp-course/chapter1/1
13. 通向AGI之路：大型语言模型（LLM）技术精要
    1. https://zhuanlan.zhihu.com/p/597586623
14. **Large Language Models: Foundation Models from the Ground Up**
    1. https://learning.edx.org/course/course-v1:Databricks+LLM102x+2T2023/home
15. **万字通俗讲解大语言模型内部运行原理**
    1. https://www.youtube.com/watch?v=dIyQl99oxlg&t=958s
    2. https://www.understandingai.org/p/large-language-models-explained-with
    3.# llm101
all in one entrance class for new learner of llm
