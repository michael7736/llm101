{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdc3329-844e-4048-a92b-350f878f59d3",
   "metadata": {},
   "source": [
    "Usage with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7201af71-e2f8-4656-93a8-ba4de1e7adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be33d50c-bdff-4b9f-b6b6-5c1146e9fc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangzg/miniconda3/envs/llm103/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/mnt/newdisk/data/models/openai-community_gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/newdisk/data/models/openai-community_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "192fe77b-f96f-436a-a771-4fa4f2d61511",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is my text examples for tokenization!\"\n",
    "prompt = \"GPT-2 is a model developed by OpenAI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f05bc7a9-5e7e-405f-b8c6-5fe31b321df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8840523e-74aa-418e-a1ca-43743ba70a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   38, 11571,    12,    17,   318,   257,  2746,  4166,   416,  4946,\n",
       "         20185,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1a94796-275c-45cc-a31b-021766aa76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de1719a9-bee7-4467-8caf-39399a6d2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "353e801c-8a6d-46e1-846a-ccd905309898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9776, 407, 900, 13, 632, 481, 307, 900, 284, 4600, 17821, 63, 416, 4277, 13, 770, 4069, 481, 307, 39224, 287, 6121, 364, 410, 19, 13, 2231, 11, 290, 481, 307, 788, 900, 284, 4600, 25101, 63, 416, 4277, 13, 1114, 517, 3307, 2198, 428, 2071, 25, 3740, 1378, 12567, 13, 785, 14, 71, 1018, 2667, 2550, 14, 35636, 364, 14, 37165, 14, 36042, 5705]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a81da-d09e-4c3c-8df4-08aa17aa72d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
